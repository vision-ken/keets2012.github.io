<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[消息中间件NSQ深入与实践]]></title>
    <url>%2F2017%2F10%2F08%2Fnsq%2F</url>
    <content type="text"><![CDATA[1. 介绍最近在研究一些消息中间件，常用的MQ如RabbitMQ,ActiveMQ,Kafka等。NSQ是一个基于Go语言的分布式实时消息平台，它基于MIT开源协议发布，由bitly公司开源出来的一款简单易用的消息中间件。官方和第三方还为NSQ开发了众多客户端功能库，如官方提供的基于HTTP的nsqd、Go客户端go-nsq、Python客户端pynsq、基于Node.js的JavaScript客户端nsqjs、异步C客户端libnsq、Java客户端nsq-java以及基于各种语言的众多第三方客户端功能库。 1.1 Features1). DistributedNSQ提供了分布式的，去中心化，且没有单点故障的拓扑结构，稳定的消息传输发布保障，能够具有高容错和HA（高可用）特性。2). Scalable易于扩展NSQ支持水平扩展，没有中心化的brokers。内置的发现服务简化了在集群中增加节点。同时支持pub-sub和load-balanced 的消息分发。3). Ops FriendlyNSQ非常容易配置和部署，生来就绑定了一个管理界面。二进制包没有运行时依赖。官方有Docker image。4.Integrated高度集成官方的 Go 和 Python库都有提供。而且为大多数语言提供了库。 1.2 组件 Topic ：一个topic就是程序发布消息的一个逻辑键，当程序第一次发布消息时就会创建topic。 Channels ：channel与消费者相关，是消费者之间的负载均衡，channel在某种意义上来说是一个“队列”。每当一个发布者发送一条消息到一个topic，消息会被复制到所有消费者连接的channel上，消费者通过这个特殊的channel读取消息，实际上，在消费者第一次订阅时就会创建channel。Channel会将消息进行排列，如果没有消费者读取消息，消息首先会在内存中排队，当量太大时就会被保存到磁盘中。 Messages：消息构成了我们数据流的中坚力量，消费者可以选择结束消息，表明它们正在被正常处理，或者重新将他们排队待到后面再进行处理。每个消息包含传递尝试的次数，当消息传递超过一定的阀值次数时，我们应该放弃这些消息，或者作为额外消息进行处理。 nsqd：nsqd 是一个守护进程，负责接收，排队，投递消息给客户端。它可以独立运行，不过通常它是由 nsqlookupd 实例所在集群配置的（它在这能声明 topics 和 channels，以便大家能找到）。 nsqlookupd：nsqlookupd 是守护进程负责管理拓扑信息。客户端通过查询 nsqlookupd 来发现指定话题（topic）的生产者，并且 nsqd 节点广播话题（topic）和通道（channel）信息。有两个接口：TCP 接口，nsqd 用它来广播。HTTP 接口，客户端用它来发现和管理。 nsqadmin：nsqadmin 是一套 WEB UI，用来汇集集群的实时统计，并执行不同的管理任务。 常用工具类： nsq_to _file：消费指定的话题（topic）/通道（channel），并写到文件中，有选择的滚动和/或压缩文件。 nsq_to _http：消费指定的话题（topic）/通道（channel）和执行 HTTP requests (GET/POST) 到指定的端点。 nsq_to _nsq：消费者指定的话题/通道和重发布消息到目的地 nsqd 通过 TCP。 1.3 拓扑结构NSQ推荐通过他们相应的nsqd实例使用协同定位发布者，这意味着即使面对网络分区，消息也会被保存在本地，直到它们被一个消费者读取。更重要的是，发布者不必去发现其他的nsqd节点，他们总是可以向本地实例发布消息。 首先，一个发布者向它的本地nsqd发送消息，要做到这点，首先要先打开一个连接，然后发送一个包含topic和消息主体的发布命令，在这种情况下，我们将消息发布到事件topic上以分散到我们不同的worker中。事件topic会复制这些消息并且在每一个连接topic的channel上进行排队，在我们的案例中，有三个channel，它们其中之一作为档案channel。消费者会获取这些消息并且上传到S3。 每个channel的消息都会进行排队，直到一个worker把他们消费，如果此队列超出了内存限制，消息将会被写入到磁盘中。Nsqd节点首先会向nsqlookup广播他们的位置信息，一旦它们注册成功，worker将会从nsqlookup服务器节点上发现所有包含事件topic的nsqd节点。 然后每个worker向每个nsqd主机进行订阅操作，用于表明worker已经准备好接受消息了。这里我们不需要一个完整的连通图，但我们必须要保证每个单独的nsqd实例拥有足够的消费者去消费它们的消息，否则channel会被队列堆着。 2. Internals2.1 消息传递担保NSQ 保证消息将交付至少一次，虽然消息可能是重复的。消费者应该关注到这一点，删除重复数据或执行idempotent等操作。这个担保是作为协议和工作流的一部分，工作原理如下（假设客户端成功连接并订阅一个话题）：1）客户表示已经准备好接收消息2）NSQ 发送一条消息，并暂时将数据存储在本地（在 re-queue 或 timeout）3）客户端回复 FIN（结束）或 REQ（重新排队）分别指示成功或失败。如果客户端没有回复, NSQ 会在设定的时间超时，自动重新排队消息这确保了消息丢失唯一可能的情况是不正常结束 nsqd 进程。在这种情况下，这是在内存中的任何信息（或任何缓冲未刷新到磁盘）都将丢失。如何防止消息丢失是最重要的，即使是这个意外情况可以得到缓解。一种解决方案是构成冗余 nsqd对（在不同的主机上）接收消息的相同部分的副本。因为你实现的消费者是幂等的，以两倍时间处理这些消息不会对下游造成影响，并使得系统能够承受任何单一节点故障而不会丢失信息。 2.2 简化配置和管理单个 nsqd 实例被设计成可以同时处理多个数据流。流被称为“话题”和话题有 1 个或多个“通道”。每个通道都接收到一个话题中所有消息的拷贝。在实践中，一个通道映射到下行服务消费一个话题。话题和通道都没有预先配置。话题由第一次发布消息到命名的话题或第一次通过订阅一个命名话题来创建。通道被第一次订阅到指定的通道创建。话题和通道的所有缓冲的数据相互独立，防止缓慢消费者造成对其他通道的积压（同样适用于话题级别）。一个通道一般会有多个客户端连接。假设所有已连接的客户端处于准备接收消息的状态，每个消息将被传递到一个随机的客户端。nsqlookupd，它提供了一个目录服务，消费者可以查找到提供他们感兴趣订阅话题的 nsqd 地址 。在配置方面，把消费者与生产者解耦开（它们都分别只需要知道哪里去连接 nsqlookupd 的共同实例，而不是对方），降低复杂性和维护。在更底的层面，每个 nsqd 有一个与 nsqlookupd 的长期 TCP 连接，定期推动其状态。这个数据被 nsqlookupd 用于给消费者通知 nsqd 地址。对于消费者来说，一个暴露的 HTTP /lookup 接口用于轮询。为话题引入一个新的消费者，只需启动一个配置了 nsqlookup 实例地址的 NSQ 客户端。无需为添加任何新的消费者或生产者更改配置，大大降低了开销和复杂性。 2.3 消除单点故障NSQ被设计以分布的方式被使用。nsqd 客户端（通过 TCP ）连接到指定话题的所有生产者实例。没有中间人，没有消息代理，也没有单点故障。这种拓扑结构消除单链，聚合，反馈。相反，你的消费者直接访问所有生产者。从技术上讲，哪个客户端连接到哪个 NSQ 不重要，只要有足够的消费者连接到所有生产者，以满足大量的消息，保证所有东西最终将被处理。对于 nsqlookupd，高可用性是通过运行多个实例来实现。他们不直接相互通信和数据被认为是最终一致。消费者轮询所有的配置的 nsqlookupd 实例和合并 response。失败的，无法访问的，或以其他方式故障的节点不会让系统陷于停顿。 2.4 效率对于数据的协议，通过推送数据到客户端最大限度地提高性能和吞吐量的，而不是等待客户端拉数据。这个概念，称之为 RDY 状态，基本上是客户端流量控制的一种形式。当客户端连接到 nsqd 和并订阅到一个通道时，它被放置在一个 RDY 为 0 状态。这意味着，还没有信息被发送到客户端。当客户端已准备好接收消息发送，更新它的命令 RDY 状态到它准备处理的数量，比如 100。无需任何额外的指令，当 100 条消息可用时，将被传递到客户端（服务器端为那个客户端每次递减 RDY 计数）。客户端库的被设计成在 RDY 数达到配置 max-in-flight 的 25% 发送一个命令来更新 RDY 计数（并适当考虑连接到多个 nsqd 情况下，适当地分配）。 2.5 心跳和超时NSQ 的 TCP 协议是面向 push 的。在建立连接，握手，和订阅后，消费者被放置在一个为 0 的 RDY 状态。当消费者准备好接收消息，它更新的 RDY 状态到准备接收消息的数量。NSQ 客户端库不断在幕后管理，消息控制流的结果。每隔一段时间，nsqd 将发送一个心跳线连接。客户端可以配置心跳之间的间隔，但 nsqd 会期待一个回应在它发送下一个心掉之前。组合应用级别的心跳和 RDY 状态，避免头阻塞现象，也可能使心跳无用（即，如果消费者是在后面的处理消息流的接收缓冲区中，操作系统将被填满，堵心跳）为了保证进度，所有的网络 IO 时间上限势必与配置的心跳间隔相关联。这意味着，你可以从字面上拔掉之间的网络连接 nsqd 和消费者，它会检测并正确处理错误。当检测到一个致命错误，客户端连接被强制关闭。在传输中的消息会超时而重新排队等待传递到另一个消费者。最后，错误会被记录并累计到各种内部指标。 2.6 分布式因为NSQ没有在守护程序之间共享信息，所以它从一开始就是为了分布式操作而生。个别的机器可以随便宕机随便启动而不会影响到系统的其余部分，消息发布者可以在本地发布，即使面对网络分区。这种“分布式优先”的设计理念意味着NSQ基本上可以永远不断地扩展，需要更高的吞吐量？那就添加更多的nsqd吧。唯一的共享状态就是保存在lookup节点上，甚至它们不需要全局视图，配置某些nsqd注册到某些lookup节点上这是很简单的配置，唯一关键的地方就是消费者可以通过lookup节点获取所有完整的节点集。清晰的故障事件——NSQ在组件内建立了一套明确关于可能导致故障的的故障权衡机制，这对消息传递和恢复都有意义。虽然它们可能不像Kafka系统那样提供严格的保证级别，但NSQ简单的操作使故障情况非常明显。 2.7 no replication不像其他的队列组件，NSQ并没有提供任何形式的复制和集群，也正是这点让它能够如此简单地运行，但它确实对于一些高保证性高可靠性的消息发布没有足够的保证。我们可以通过降低文件同步的时间来部分避免，只需通过一个标志配置，通过EBS支持我们的队列。但是这样仍然存在一个消息被发布后马上死亡，丢失了有效的写入的情况。 2.8 没有严格的顺序虽然Kafka由一个有序的日志构成，但NSQ不是。消息可以在任何时间以任何顺序进入队列。在我们使用的案例中，这通常没有关系，因为所有的数据都被加上了时间戳，但它并不适合需要严格顺序的情况。 2.9 无数据重复删除功能NSQ对于超时系统，它使用了心跳检测机制去测试消费者是否存活还是死亡。很多原因会导致我们的consumer无法完成心跳检测，所以在consumer中必须有一个单独的步骤确保幂等性。 3. 实践安装过程本文将nsq集群具体的安装过程略去，大家可以自行参考官网，比较简单。这部分介绍下笔者实验的拓扑，以及nsqadmin的相关信息。 3.1 拓扑结构 实验采用3台NSQD服务，2台LOOKUPD服务。采用官方推荐的拓扑，消息发布的服务和NSQD在一台主机。一共5台机器。NSQ基本没有配置文件，配置通过命令行指定参数。主要命令如下:LOOKUPD命令 1bin/nsqlookupd NSQD命令 1bin/nsqd --lookupd-tcp-address=172.16.30.254:4160 -broadcast-address=172.16.30.254 1bin/nsqadmin --lookupd-http-address=172.16.30.254:4161 工具类，消费后存储到本地文件。 1bin/nsq_to_file --topic=newtest --channel=test --output-dir=/tmp --lookupd-http-address=172.16.30.254:4161 发布一条消息1curl -d 'hello world 5' 'http://172.16.30.254:4151/put?topic=test' 3.2 nsqadmin对Streams的详细信息进行查看，包括NSQD节点，具体的channel，队列中的消息数，连接数等信息。 列出所有的NSQD节点: 消息的统计: lookup主机的列表: 4. 总结NSQ基本核心就是简单性，是一个简单的队列，这意味着它很容易进行故障推理和很容易发现bug。消费者可以自行处理故障事件而不会影响系统剩下的其余部分。 事实上，简单性是我们决定使用NSQ的首要因素，这方便与我们的许多其他软件一起维护，通过引入队列使我们得到了堪称完美的表现，通过队列甚至让我们增加了几个数量级的吞吐量。越来越多的consumer需要一套严格可靠性和顺序性保障，这已经超过了NSQ提供的简单功能。 结合我们的业务系统来看，对于我们所需要传输的发票消息，相对比较敏感，无法容忍某个nsqd宕机，或者磁盘无法使用的情况，该节点堆积的消息无法找回。这是我们没有选择该消息中间件的主要原因。简单性和可靠性似乎并不能完全满足。相比Kafka，ops肩负起更多负责的运营。另一方面，它拥有一个可复制的、有序的日志可以提供给我们更好的服务。但对于其他适合NSQ的consumer，它为我们服务的相当好，我们期待着继续巩固它的坚实的基础。 ps: 本文首发于笔者的csdn博客，此处将其加入个人的博客。 参考 NSQ：分布式的实时消息平台 NSQ - NYC Golang Meetup NSQ Docs]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Cluster</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok使用与原理]]></title>
    <url>%2F2017%2F10%2F06%2Flombok%2F</url>
    <content type="text"><![CDATA[1. Lombok简介首先Lombok是一款Java IDE的应用工具插件，一个可以通过简单的注解形式来帮助我们简化消除一些必须有但显得很臃肿的Java代码的工具，比如属性的构造器、getter、setter、equals、hashcode、toString方法。结合IDE，通过使用对应的注解，可以在编译源码的时候生成对应的方法。官方地址：https://projectlombok.org/。 虽然上述的那些常用方法IDE都能生成，但是lombok更加简洁与方便，能够达到的效果就是在源码中不需要写一些通用的方法，但是在编译生成的字节码文件中会帮我们生成这些方法，这就是lombok的神奇作用。 2. 安装2.1 插件安装笔者主要使用的IDE是Intellij idea，编译器需要在 1preference-&gt;plugins-&gt;Browse repositories 搜索lombok，然后安装plugins，需要稍等片刻。笔者截图已经安装好。 2.2 添加jar包在项目中添加lombok的jar包，笔者用的是maven，所以在pom文件中添加了如下的依赖。gradle使用见官网。 123456&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.16&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 3. 使用lombok主要通过注解起作用，详细的注解见Lombok features。 With Lombok: 12345678910111213import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import java.io.Serializable;@Data@AllArgsConstructorpublic class UserEntity implements Serializable &#123; private long userId; private String userName; private String sex;&#125; 编译后： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import java.beans.ConstructorProperties;import java.io.Serializable;public class UserEntity implements Serializable &#123; private long userId; private String userName; private String sex; public long getUserId() &#123; return this.userId; &#125; public String getUserName() &#123; return this.userName; &#125; public String getSex() &#123; return this.sex; &#125; public void setUserId(long userId) &#123; this.userId = userId; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public boolean equals(Object o) &#123; if(o == this) &#123; return true; &#125; else if(!(o instanceof UserEntity)) &#123; return false; &#125; else &#123; UserEntity other = (UserEntity)o; if(!other.canEqual(this)) &#123; return false; &#125; else if(this.getUserId() != other.getUserId()) &#123; return false; &#125; else &#123; Object this$userName = this.getUserName(); Object other$userName = other.getUserName(); if(this$userName == null) &#123; if(other$userName != null) &#123; return false; &#125; &#125; else if(!this$userName.equals(other$userName)) &#123; return false; &#125; Object this$sex = this.getSex(); Object other$sex = other.getSex(); if(this$sex == null) &#123; if(other$sex != null) &#123; return false; &#125; &#125; else if(!this$sex.equals(other$sex)) &#123; return false; &#125; return true; &#125; &#125; &#125; protected boolean canEqual(Object other) &#123; return other instanceof UserEntity; &#125; public int hashCode() &#123; int PRIME = true; int result = 1; long $userId = this.getUserId(); int result = result * 59 + (int)($userId &gt;&gt;&gt; 32 ^ $userId); Object $userName = this.getUserName(); result = result * 59 + ($userName == null?43:$userName.hashCode()); Object $sex = this.getSex(); result = result * 59 + ($sex == null?43:$sex.hashCode()); return result; &#125; public String toString() &#123; return "UserEntity(userId=" + this.getUserId() + ", userName=" + this.getUserName() + ", sex=" + this.getSex() + ")"; &#125; @ConstructorProperties(&#123;"userId", "userName", "sex"&#125;) public UserEntity(long userId, String userName, String sex) &#123; this.userId = userId; this.userName = userName; this.sex = sex; &#125;&#125; 这边介绍笔者经常使用到的注解。 val，用在局部变量前面，相当于将变量声明为final @Value用在类上，是@Data的不可变形式，相当于为属性添加final声明，只提供getter方法，而不提供setter方法 @Data@ToString, @EqualsAndHashCode, 所有属性的@Getter, 所有non-final属性的@Setter和@RequiredArgsConstructor的组合，通常情况下，我们使用这个注解就足够了。 @NoArgsConstructor无参构造器 @AllArgsConstructor全参构造器 @ToString 生成toString方法，默认情况下，会输出类名、所有属性，属性会按照顺序输出，以逗号分割。 @EqualsAndHashCode默认情况下，会使用所有非瞬态(non-transient)和非静态(non-static)字段来生成equals和hascode方法，也可以指定具体使用哪些属性。 @Getter / @Setter上面已经说过，一般用@data就不用额外加这个注解了。可以作用在类上和属性上，放在类上，会对所有的非静态(non-static)属性生成Getter/Setter方法，放在属性上，会对该属性生成Getter/Setter方法。并可以指定Getter/Setter方法的访问级别。 @NonNull，给方法参数增加这个注解会自动在方法内对该参数进行是否为空的校验，如果为空，则抛出NPE（NullPointerException） @Cleanup自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成try-finally这样的代码来关闭流 @Log根据不同的注解生成不同类型的log对象，但是实例名称都是log，有7种可选实现类： 1). @Log4j 1private static final org.apache.log4j.Logger log = org.apache.log4j.Logger.getLogger(LogExample.class); 2). @Log4j2 1private static final org.apache.logging.log4j.Logger log = org.apache.logging.log4j.LogManager.getLogger(LogExample.class); 3). @Slf4j 1private static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(LogExample.class); 4). @XSlf4j 1private static final org.slf4j.ext.XLogger log = org.slf4j.ext.XLoggerFactory.getXLogger(LogExample.class); 5). @CommonsLog 1private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(LogExample.class); 6). @JBossLog 1private static final org.jboss.logging.Logger log = org.jboss.logging.Logger.getLogger(LogExample.class); 7). @Log private static final java.util.logging.Logger log = java.util.logging.Logger.getLogger(LogExample.class.getName()); 默认情况下，logger的名字将会是被@Log注解的那个类的名字。当然这也可以被个性化命名，通过topic参数，如@XSlf4j(topic=&quot;reporting&quot;)。 4. 原理lombok 主要通过注解生效，自jdk5引入注解，由两种解析方式。第一种是运行时解析，@Retention(RetentionPolicy.RUNTIME), 定义注解的保留策略，这样可以通过反射拿到该注解。另一种是编译时解析，有两种机制。 Annotation Processing Tool，apt自JDK5产生，JDK7已标记为过期，不推荐使用，JDK8中已彻底删除，自JDK6开始，可以使用Pluggable Annotation Processing API来替换它，apt被替换主要有2点原因。api都在com.sun.mirror非标准包下，还有就是没有集成到javac中，需要额外运行。 Pluggable Annotation Processing APIlombok使用这种方式实现，基于JSR 269，自JDK6加入，作为apt的替代方案，它解决了apt的两个问题，javac在执行的时候会调用实现了该API的程序，这样我们就可以对编译器做一些增强，这时javac执行的过程如下： 5. 总结这篇文章主要讲解了lombok的入门与使用。介绍了一些常用的lombok注解，大大简化了我们的开发工作和代码的简洁性。当然，lombok不支持多种参数构造器的重载，工具毕竟是工具，我感觉并不会有非常完美适合每个人的工具。最后，我个人还是很推荐这款插件的，毕竟我很懒，😆。 参考 Lombok Docs Java奇淫巧技之Lombok]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Cluster深入与实践]]></title>
    <url>%2F2017%2F09%2F29%2Frediscluster%2F</url>
    <content type="text"><![CDATA[1. redis介绍www.redis.ioredis是一个基于内存的K-V存储数据库。支持存储的类型有string,list,set,zset(sorted set),hash等。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。redis支持各种不同方式的排序。保证效率的情况下，数据缓存在内存中。同时redis提供了持久化策略，不同的策略触发同步到磁盘或者把修改操作写入追加的记录文件，在此基础上实现了master-slave。 它是一个高性能的存储系统，能支持超过 100K+ 每秒的读写频率。同时还支持消息的发布/订阅，从而让你在构建高性能消息队列系统时多了另一种选择。 Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。这使得Redis可执行单层树复制。存盘可以有意无意的对数据进行写操作。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助。 2. 主从redis支持master-slave模式，一主多从，redis server可以设置另外多个redis server为slave，从机同步主机的数据。配置后，读写分离，主机负责读写服务，从机只负责读。减轻主机的压力。redis实现的是最终会一致性，具体选择强一致性还是弱一致性，取决于业务场景。redis 主从同步有两种方式（或者所两个阶段）：全同步和部分同步。 主从刚刚连接的时候，进行全同步；全同步结束后，进行部分同步。当然，如果有需要，slave 在任何时候都可以发起全同步。redis 策略是，无论如何，首先会尝试进行部分同步，如不成功，要求从机进行全同步，并启动 BGSAVE……BGSAVE 结束后，传输 RDB 文件；如果成功，允许从机进行部分同步，并传输积压空间中的数据。简单来说，主从同步就是 RDB 文件的上传下载；主机有小部分的数据修改，就把修改记录传播给每个从机。 3. redis集群主从模式存在的问题是，master宕机之后，从机只能读，不可写，不能保证高可用。redis集群技术是构建高性能网站架构的重要手段，试想在网站承受高并发访问压力的同时，还需要从海量数据中查询出满足条件的数据，并快速响应，我们必然想到的是将数据进行切片，把数据根据某种规则放入多个不同的服务器节点，来降低单节点服务器的压力。 Redis Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。节点之间使用gossip协议传播信息以及发现新节点。 Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现，集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。 Redis 集群中不存在中心（central）节点或者代理（proxy）节点，集群的其中一个主要设计目标是达到线性可扩展性（linear scalability）。 Redis 集群为了保证一致性（consistency）而牺牲了一部分容错性：系统会在保证对网络断线（net split）和节点失效（node failure）具有有限（limited）抵抗力的前提下，尽可能地保持数据的一致性。 4. 安装部署redis安装较为简单，官网下载压缩包解压。集群模式需要ruby的编译环境，集群最小的配置为3台master，小于3则启动集群报错。redis版本：3.2.4 4.1 主从模式拓扑图 主从模式采用一主三从，主从都配置auth认证，读写分离。主要实验的动作：1）多个app 同时写，测定写速率；2）多个app 同时写，同时有读的进程，测定读写速率；3）master主机宕机，app依然进行读写。 4.2 cluster拓扑图如下 集群模式采用四主四从，也是采用读写分离。主要实验的动作：1）有一个master宕机，观察日志，新的slave成为master；2）master宕机后，重新启动，master成为slave；3）集群全部宕机，redis主机重启，数据未丢失。 5. 原理5.1 一致性filesnapshot:默认redis是会以快照的形式将数据持久化到磁盘,在配置文件中的格式是：save N M表示在N秒之内，redis至少发生M次修改则redis抓快照到磁盘。 工作原理：当redis需要做持久化时，redis会fork一个子进程；子进程将数据写到磁盘上一个临时RDB文件中；当子进程完成写临时文件后，将原来的RDB替换掉，这样的好处就是可以copy-on-write。 Append-only：filesnapshotting方法在redis异常死掉时， 最近的数据会丢失（丢失数据的多少视你save策略的配置），所以这是它最大的缺点，当业务量很大时，丢失的数据是很多的。Append-only方法可 以做到全部数据不丢失，但redis的性能就要差些。AOF就可以做到全程持久化，只需要在配置文件中开启（默认是no），appendonly yes开启AOF之后，redis每执行一个修改数据的命令，都会把它添加到aof文件中，当redis重启时，将会读取AOF文件进行“重放”以恢复到 redis关闭前的最后时刻。 AOF文件刷新的方式，有三种，参考配置参数appendfsync ： appendfsync always每提交一个修改命令都调用fsync刷新到AOF文件，非常非常慢，但也非常安全； appendfsync everysec每秒钟都调用fsync刷新到AOF文件，很快，但可能会丢失一秒以内的数据； appendfsync no依靠OS进行刷新，redis不主动刷新AOF，这样最快，但安全性就差。默认并推荐每秒刷新，这样在速度和安全上都做到了兼顾。 Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。 Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据。 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。 Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。Redis在master是非阻塞模式，也就是说在slave执行数据同步的时候，master是可以接受客户端的请求的，并不影响同步数据的一致性，然而在slave端是阻塞模式的，slave在同步master数据时，并不能够响应客户端的查询。 5.2 Replication的工作原理(1)Slave服务器连接到Master服务器。(2)Slave服务器发送SYCN命令。(3)Master服务器备份数据库到.rdb文件。(4)Master服务器把.rdb文件传输给Slave服务器。(5)Slave服务器把.rdb文件数据导入到数据库中。 在Slave启动并连接到Master之后，它将主动发送一个SYNC命令。此后Master将启动后台存盘进程，同时收集所有接收到的用于修改数据集 的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave服务器在接收到数据库文件数据之后将其 存盘并加载到内存中。此后，Master继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。 5.3 一致性哈希集群要实现的目的是要将不同的 key 分散放置到不同的 redis 节点，这里我们需要一个规则或者算法，通常的做法是获取 key 的哈希值，然后根据节点数来求模，但这种做法有其明显的弊端，当我们需要增加或减少一个节点时，会造成大量的 key 无法命中，这种比例是相当高的，所以就有人提出了一致性哈希的概念。一致性哈希有四个重要特征： 均衡性：也有人把它定义为平衡性，是指哈希的结果能够尽可能分布到所有的节点中去，这样可以有效的利用每个节点上的资源。 单调性：对于单调性有很多翻译让我非常的不解，而我想要的是当节点数量变化时哈希的结果应尽可能的保护已分配的内容不会被重新分派到新的节点。 分散性和负载：这两个其实是差不多的意思，就是要求一致性哈希算法对 key 哈希应尽可能的避免重复。 Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。 使用哈希槽的好处就在于可以方便的添加或移除节点。 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了； 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了； 当设置了主从关系后，slave 在第一次连接或者重新连接 master 时，slave 都会发送一条同步指令给 master； master 接到指令后，开始启动后台保存进程保存数据，接着收集所有的数据修改指令。后台保存完了，master 就把这份数据发送给 slave，slave 先把数据保存到磁盘，然后把它加载到内存中，master 接着就把收集的数据修改指令一行一行的发给 slave，slave 接收到之后重新执行该指令，这样就实现了数据同步。 slave 在与 master 失去联系后，自动的重新连接。如果 master 收到了多个 slave 的同步请求，它会执行单个后台保存来为所有的 slave 服务。 5.4 节点失效检测以下是节点失效检查的实现方法： 当一个节点向另一个节点发送 PING 命令，但是目标节点未能在给定的时限内返回 PING 命令的回复时，那么发送命令的节点会将目标节点标记为PFAIL （possible failure，可能已失效）。 等待 PING 命令回复的时限称为“节点超时时限（node timeout）”，是一个节点选项（node-wise setting）。 每次当节点对其他节点发送 PING 命令的时候，它都会随机地广播三个它所知道的节点的信息，这些信息里面的其中一项就是说明节点是否已经被标记为 PFAIL 或者 FAIL 。 当节点接收到其他节点发来的信息时，它会记下那些被其他节点标记为失效的节点。这称为失效报告（failure report）。 如果节点已经将某个节点标记为 PFAIL ，并且根据节点所收到的失效报告显式，集群中的大部分其他主节点也认为那个节点进入了失效状态，那么节点会将那个失效节点的状态标记为 FAIL 。 一旦某个节点被标记为 FAIL ，关于这个节点已失效的信息就会被广播到整个集群，所有接收到这条信息的节点都会将失效节点标记为 FAIL 。 简单来说，一个节点要将另一个节点标记为失效，必须先询问其他节点的意见，并且得到大部分主节点的同意才行。因为过期的失效报告会被移除，所以主节点要将某个节点标记为 FAIL 的话，必须以最近接收到的失效报告作为根据。在以下两种情况中，节点的 FAIL 状态会被移除： 如果被标记为 FAIL 的是从节点，那么当这个节点重新上线时， FAIL 标记就会被移除。保持（retaning）从节点的 FAIL 状态是没有意义的，因为它不处理任何槽，一个从节点是否处于 FAIL 状态，决定了这个从节点在有需要时能否被提升为主节点。 如果一个主节点被打上 FAIL 标记之后，经过了节点超时时限的四倍时间，再加上十秒钟之后，针对这个主节点的槽的故障转移操作仍未完成，并且这个主节点已经重新上线的话，那么移除对这个节点的 FAIL 标记。 在第二种情况中，如果故障转移未能顺利完成，并且主节点重新上线，那么集群就继续使用原来的主节点，从而免去管理员介入的必要。 5.5 从节点选举一旦某个主节点进入 FAIL 状态，如果这个主节点有一个或多个从节点存在，那么其中一个从节点会被升级为新的主节点，而其他从节点则会开始对这个新的主节点进行复制。新的主节点由已下线主节点属下的所有从节点中自行选举产生，以下是选举的条件： 这个节点是已下线主节点的从节点。 已下线主节点负责处理的槽数量非空。 从节点的数据被认为是可靠的，也即是，主从节点之间的复制连接（replication link）的断线时长不能超过节点超时时限（node timeout）乘以REDIS_CLUSTER_SLAVE_VALIDITY_MULT 常量得出的积。 如果一个从节点满足了以上的所有条件，那么这个从节点将向集群中的其他主节点发送授权请求，询问它们，是否允许自己（从节点）升级为新的主节点。如果发送授权请求的从节点满足以下属性，那么主节点将向从节点返回 FAILOVER_AUTH_GRANTED 授权，同意从节点的升级要求： 发送授权请求的是一个从节点，并且它所属的主节点处于 FAIL 状态。 在已下线主节点的所有从节点中，这个从节点的节点 ID 在排序中是最小的。 从节点处于正常的运行状态：它没有被标记为 FAIL 状态，也没有被标记为 PFAIL 状态。 一旦某个从节点在给定的时限内得到大部分主节点的授权，它就会开始执行以下故障转移操作： 通过 PONG 数据包（packet）告知其他节点，这个节点现在是主节点了。 通过 PONG 数据包告知其他节点，这个节点是一个已升级的从节点（promoted slave）。 接管（claiming）所有由已下线主节点负责处理的哈希槽。 显式地向所有节点广播一个 PONG 数据包，加速其他节点识别这个节点的进度，而不是等待定时的 PING / PONG 数据包。 所有其他节点都会根据新的主节点对配置进行相应的更新，特别地： a. 所有被新的主节点接管的槽会被更新。 b. 已下线主节点的所有从节点会察觉到 PROMOTED 标志，并开始对新的主节点进行复制。 c.如果已下线的主节点重新回到上线状态，那么它会察觉到 PROMOTED 标志，并将自身调整为现任主节点的从节点。 在集群的生命周期中，如果一个带有 PROMOTED 标识的主节点因为某些原因转变成了从节点，那么该节点将丢失它所带有的 PROMOTED 标识。 6. 总结Redis集群具有高可用，易于迁移，存取速度快等特点。也可以作为消息队列使用，支持pub/sub模式，具体优缺点总结如下：首先优点: redis 在主节点下线后，从节点会自动提升为主节点，提供服务 redis 宕机节点恢复后，自动会添加到集群中，变成从节点 动态扩展和删除节点，rehash slot的分配，基于桶的数据分布方式大大降低了迁移成本，只需将数据桶从一个Redis Node迁移到另一个Redis Node即可完成迁移。 Redis Cluster使用异步复制。 其缺点为: 由于redis的复制使用异步机制，在自动故障转移的过程中，集群可能会丢失写命令。然而 redis 几乎是同时执行(将命令恢复发送给客户端，以及将命令复制到从节点)这两个操作，所以实际中，命令丢失的窗口非常小。 普通的主从模式支持auth加密认证，虽然比较弱，但写或者读都要通过密码验证，cluster对密码支持不太友好，如果对集群设置密码，那么requirepass和masterauth都需要设置，否则发生主从切换时，就会遇到授权问题，可以模拟并观察日志。 参考资料： www.redis.io redis-cluster研究和使用 Redis Cluster 3.0搭建与使用]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由Consul谈到Raft]]></title>
    <url>%2F2017%2F09%2F25%2Fraft%2F</url>
    <content type="text"><![CDATA[在前一篇文章consul配置与实战中，介绍了consul的一些内幕及consul配置相关，并对项目中的一些实际配置进行展示。这篇文章重点介绍consul中所涉及到的一致性算法raft。 1. 背景分布式系统的一致性是相当重要的，即为CAP理论中的C(Consistency)。一致性又可以分为强一致性和最终一致性。这篇文章重点讨论强一致性算法raft。 Lamport发表Paxos一致性算法从90年提出到现在已经有二十几年了，直到2006年Google的三篇论文初现“云”的端倪，其中的Chubby Lock服务使用Paxos作为Chubby Cell中的一致性算法，Paxos的人气从此一路狂飙。而Paxos流程太过于繁杂实现起来也比较复杂，虽然现在很广泛使用的Zookeeper也是基于Paxos算法来实现，但是Zookeeper使用的ZAB（Zookeeper Atomic Broadcast）协议对Paxos进行了很多的改进与优化，复杂性是制约他发展的一个重要原因。Raft的设计初衷就是易于理解性。 Raft是斯坦福的Diego Ongaro、John Ousterhout两个人以易懂（Understandability）为目标设计的一致性算法，在2013年发布了论文：《In Search of an Understandable Consensus Algorithm》 从2013年发布到现在不过只有两年，到现在已经有了十多种语言的Raft算法实现框架，较为出名的有etcd。 2. Raft详解强调的是易懂（Understandability），Raft和Paxos一样只要保证n/2+1节点正常就能够提供服务；众所周知但问题较为复杂时可以把问题分解为几个小问题来处理，Raft也使用了分而治之的思想把算法流程分为三个子问题：选举（Leader election）、日志复制（Log replication）、安全性（Safety）三个子问题。 2.1 raft基本概念 states 一个raft集群拥有多个server，通常会有5台，这样可以允许系统中两台server宕机。在任何情况下，所有的server只有如下三种状态之一： Leader，负责Client交互和log复制，同一时刻系统中最多存在1个 Follower，被动响应请求RPC，从不主动发起请求RPC Candidate，由Follower向Leader转换的中间状态 在正常的操作流程中，集群中有且只有一个server，其他所有的server都是follower。follower是被动的，他们只是被动地相应Candidate和leader的请求。。leader处理所有的客户端请求，follower自己不处理而是转发给leader。第三种状态是Candidate，用来选举一个新的leader。 Terms Raft将时间划分成任意的长度周期。Terms可以理解为逻辑周期，用连续的整数表示。在分布式环境中，时间同步很重要，同时是一个难题。在Raft中使用了一个可以理解为周期（任期）的概念，用Term作为一个周期，每个Term都是一个连续递增的编号，每一轮选举都是一个Term周期，在一个Term中只能产生一个Leader。 每个term伴随着一次election，一个或多个Candidate试图成为leader，如上图的状态转换。如果某个Candidate赢得了这次election，它将升级为剩余server的leader。在某些election的情形中，会产生耗票（Split Votes）的结果 ，即投票结果无效，随后一次新的term开始。raft确保在某个term至多有一个leader。 如上图所示，时间被划分成多个terms，每个term随着一次election。election完成后，一个leader节点管理整个集群，直至这个term结束。有些election失败了，未能产生一个leader。 2.2 Leader election所有节点都是以follower启动。一个最小的 Raft 集群需要三个参与者，这样才可能投出多数票。初始状态 都是 Follower，然后发起选举这时有三种可能情形发生。如果每方都投给了自己，结果没有任何一方获得多数票。之后每个参与方随机休息一阵（Election Timeout）重新发起投票直到一方获得多数票。这里的关键就是随机 timeout，最先从timeout中恢复发起投票的一方向还在 timeout 中的另外两方请求投票，这时它们就只能投给对方了，很快达成一致。Raft的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为Follower某个节点定时器触发选举后Term递增，状态由Follower转为Candidate，向其他节点发起RequestVote RPC请求，这时候有三种可能的情况发生： 1.该RequestVote请求接收到n/2+1（过半数）个节点的投票，从Candidate转为Leader，向其他节点发送heartBeat以保持Leader的正常运转。 2.在此期间如果收到其他节点发送过来的AppendEntries RPC请求，如该节点的Term大则当前节点转为Follower，否则保持Candidate拒绝该请求。 3.Election timeout发生则Term递增，重新发起选举。 在一个Term期间每个节点只能投票一次，所以当有多个Candidate存在时就会出现每个Candidate发起的选举都存在接收到的投票数都不过半的问题，这时每个Candidate都将Term递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次存在有多个Candidate同时发起投票的问题。 引用一张网上的图片，比较形象，如下图。 2.3 Log replication日志复制主要是用于保证节点的一致性，这阶段所做的操作也是为了保证一致性与高可用性；当Leader选举出来后便开始负责客户端的请求，所有事务（更新操作）请求都必须先经过Leader处理，这些事务请求或说成命令也就是这里说的日志，我们都知道要保证节点的一致性就要保证每个节点都按顺序执行相同的操作序列，日志复制（Log Replication）就是为了保证执行相同的操作序列所做的工作；在Raft中当接收到客户端的日志（事务请求）后先把该日志追加到本地的Log中，然后通过heartbeat把该Entry同步给其他Follower，Follower接收到日志后记录日志然后向Leader发送ACK，当Leader收到大多数（n/2+1）Follower的ACK信息后将该日志设置为已提交并追加到本地磁盘中，通知客户端并在下个heartbeat中Leader将通知所有的Follower将该日志存储在自己的本地磁盘中。 上图中，当leader选出来之后，follower的logs场景很可能出现在上图中。follower有可能丢失entries、有未提交的entries、有额外的entries等等场景。Raft中，leader通过强制followers复制自己的logs来处理不一致。这意味着，在follower中logs冲突的entries将会被leader logs中的覆写。 2.4 Safety安全性是用于保证每个节点都执行相同序列的安全机制，如当某个Follower在当前Leader commit Log时变得不可用了，稍后可能该Follower又会倍选举为Leader，这时新Leader可能会用新的Log覆盖先前已committed的Log，这就是导致节点执行不同序列；Safety就是用于保证选举出来的Leader一定包含先前 commited Log的机制； Election Safety每个Term只能选举出一个Leader，假设某个Term同时选举产生两个LeaderA和LeaderB，根据选举过程定义，A和B必须同时获得超过半数节点的投票，至少存在节点N同时给予A和B投票，因此矛盾。 Leader Completeness这里所说的完整性是指Leader日志的完整性，当Log在Term1被Commit后，那么以后Term2、Term3…等的Leader必须包含该Log；Raft在选举阶段就使用Term的判断用于保证完整性：当请求投票的该Candidate的Term较大或Term相同Index更大则投票，否则拒绝该请求； Leader Append-OnlyLeader从不“重写”或者“删除”本地Log，仅仅“追加”本地Log。Raft算法中Leader权威至高无上，当Follower和Leader产生分歧的时候，永远是Leader去覆盖修正Follower。 Log Matching如果两个节点上的日志项拥有相同的Index和Term，那么这两个节点[0, Index]范围内的Log完全一致。 State Machine Safety一旦某个server将某个日志项应用于本地状态机，以后所有server对于该偏移都将应用相同日志项。 3. 总结本文主要讲解了Raft算法的基本概念，以及算法中涉及到的leader选举，日志同步，安全性。Raft是以易理解性作为其设计的一个目标，对于一个学习的新手来说，确实比Paxos易于理解很多。虽然 Raft 的论文比 Paxos 简单版论文容易读，论文依然有很多地方需要深刻体会与理解，笔者也还是搞了好几天。 参考 Raft Animate Demo Raft Paper Raft Website Raft 为什么是更易理解的分布式一致性算法 Raft一致性算法]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>consul</tag>
        <tag>Cluster</tag>
        <tag>Consensus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[consul配置与实战]]></title>
    <url>%2F2017%2F09%2F16%2Fconsul%2F</url>
    <content type="text"><![CDATA[上一篇提到，项目用的分布式服务发现与注册组件是consul，这篇文章主要来讲下consul组件在项目中的应用以及相关介绍。本文以官方文档为主要参考consul文档。 1. consul介绍consul是一个服务管理软件，主要功能如下： 支持多数据中心下，分布式高可用的，服务发现和配置共享。 consul支持健康检查，允许存储键值对。 一致性协议采用Raft算法,用来保证服务的高可用。 成员管理和消息广播采用GOSSIP协议，支持ACL访问控制。 1.1 服务注册与发现服务注册是一个服务将其位置信息在“中心注册节点”注册的过程。该服务一般会将它的主机IP地址以及端口号进行注册，有时也会有服务访问的认证信息，使用协议，版本号，以及关于环境的一些细节信息。而服务发现可以让一个应用或者组件发现其运行环境以及其它应用或组件的信息。用户配置一个服务发现工具就可以将实际容器跟运行配置分离开。常见配置信息包括：ip、端口号、名称等。 在传统情况下，当出现服务存在于多个主机节点上时，都会使用静态配置的方法来实现服务信息的注册。而当在一个复杂的系统里，需要较强的可扩展性时，服务被频繁替换时，为避免服务中断，动态的服务注册和发现就很重要。服务注册与发现的组件有很多，如Zookeeper、Etcd等。既可用于服务间的协调，同时又可用于服务的注册。 1.2 Consensus Protocol - Raft Consul使用Consensus协议Raft提供一致性（Consistency）。本文只是简单介绍在consul中的一致性，后面专门一篇写raft。 首先，Raft是一种基于Paxos的Consensus算法。相比于Paxos，Raft设计采用了较少的状态，并且是一种更简单、更易于理解的算法。只有Server节点参与Raft，且是peer set的一员。所有的Client节点只是转发请求到Server。这种设计的考虑是，当更多的成员加入到peer set中时，quorum的规模也会增加。可能会导致性能问题是等待quorum个节点log entry。 启动Consul时，单个consul节点需要以bootstrap模式运行，该模式运行自我选举为leader。一旦Leader被选出来，其他Server可以添加Peer set中，保持一致性和安全性。最终一些Server添加到集群，bootstrap模式需要禁用。 因为所有Server都是Peer set中的成员，它们都知道谁是Leader。当一个RPC请求到达某个非Leader Server节点，请求就会被转发到Leader。如果RPC是一种query类型，这意味着它是只读的，Leader会基于FSM当前生成相应的结果，如果RPC是一种transaction类型，即修改状态，Leader产生一个新的日志条目，并基于Raft算法进行管理。一旦日志条目应用于有限状态机，transaction完成。 由于Raft的replication性质，性能对网络延迟是非常敏感的。为此，每个数据中心选择独立的Leader和维护一个不关联的peer set。数据按照数据中心进行划分，所以每个Leader只负责在相应数据中心的数据。当接收到一个远程数据中心的请求时，请求会被转发到相应的Leader。这种设计在不牺牲一致性的情况实现较低延迟交易和更高的可用性。虽然所有日志副本的写入都是基于Raft，读取更灵活。但为了支持开发人员可能需要的各种权衡，Consul支持3种不同的一致性模式。 Default，Raft采用Leader租赁模式，提供了一个时间窗口，在该时间段内，Leader角色是稳定的。 consistent，无条件一致性 stale，这种模式允许在任何Server节点执行读取操作，无论它是不是Leader。 1.3 Group Membership Protocol - GossipConsul使用gossip协议管理成员关系、广播消息到整个集群。详情可参考Serf library。 Consul利用两个不同的gossip pool。局域网(LAN Pool)和广域网(WAN Pool)。 每个Consul数据中心都有一个包含所有成员（Server和Client）的LAN gossip pool。LAN Pool有如下几个目的： 首先，成员关系允许Client自动发现Server节点，减少所需的配置量。 其次，分布式故障检测允许的故障检测的工作在某几个Server几点执行，而不是集中整个集群所有节点上。 最后，gossip允许可靠和快速的事件广播，如Leader选举。 WAN Pool是全局唯一的，无论属于哪一个数据中心，所有Server应该加入到WAN Pool。由WAN Pool提供会员信息让Server可节电执行跨数据中心的请求。集成中故障检测允许Consul妥善处理整个数据中心失去连接，或在远程数据中心只是单个的Server节点。所有这些功能都是通过利用Serf提供。从用户角度来看，它是作为一个嵌入式库提供这些功能。但其被Consul屏蔽，用户无需关心。作为开发人员可以去了解这个库是如何利用。 1.4 Session会话上一篇文章snowflake升级版全局id生成中使用到了consul的KV存储。Consul提供session会话机制，可以用于构建分布式锁。session可以绑定到节点、健康检查、KV数据，目的是提供细粒度锁。KV存储和会话的集成是使用会话的主要场景。必须在使用之前创建一个会话，然后使用它的ID。KV API支持acquire和release操作，acquire操作类似CAS操作，只有当锁空闲时才会返回成功。当成功时，某个normal标识会更新，也会递增LockIndex，当然也会更新session的信息。如果在acquire操作时，与session相关的锁已经持有，那么LockIndex就不会递增，但是key值会更新，这就允许锁的当前持有者无需重新获得锁就可以更新key的内容。 一旦获得锁，所需要经release操作来释放（使用相同的session）。Release操作也类似于CAS操作。如果给定的session无效，那么请求会失败。需要特别注意的是，无需经过session的创建者，lock也是可以被释放的。这种设计是允许操作者干预来终止会话，在需要的时候。如上所述，会话无效也将导致所有被持有的锁被释放或删除。当锁被释放时，LockIndex不会变化，但是session会被清空，并且ModifyIndex递增。这些语义允许元组（Key，LockIndex，Session）作为一个独特的“序列”。这个序列可以被传递和用于验证请求是否属于当前的锁持有者。因为每次acquire 都会导致LockIndex递增，即使同一会话中重新获取锁，该序列能够检测到陈旧的请求。同样，如果会话失效，相应的LockIndex将为空。要清楚的是，这种锁系统是纯粹的咨询。并不是强制Client必须获取锁再能执行操作作。任何客户端都可以在未获得锁的情况下读取、写入和删除Key操作。它不是Consul用于保护系统的方法。 2. consul架构上面介绍了consul的技术内幕。现在来讲讲consul的架构。 拆解开这个体系，从每一个组件开始了解。首先，可以看到有两个数据中心，分别标记为“one”和“two”。Consul是支持多数据中心一流，并且是常用业务场景。 每个数据中心都是由Server和client组成。建议有3~5台Server，基于故障处理和性能的平衡之策。如果增加越多的机器，则Consensus会越来越慢。对client没有限制，可以很容易地扩展到成千上万或数万。同一个数据中心的所有节点都要加入Gossip协议。这意味着gossip pool包含给定数据中心的所有节点。有以下目的：首先，没有必要为client配置服务器地址参数；发现是自动完成的。第二，节点故障检测的工作不是放置在服务器上，而是分布式的。这使故障检测比心跳机制更可扩展性。第三，可用来作为消息层通知重要的事件，如leader选举。 每个数据中心的服务器都是属于一个Raft peer。这意味着，他们一起工作，选出一个的Leader，Leader server是有额外的职责。负责处理所有的查询和事务。事务也必须通过Consensus协议复制到所有的伙伴。由于这一要求，当非Leader Server接收到一个RPC请求，会转发到集群的leader。 Server节点也是作为WAN gossip pool的一部分。这个pool是与LAN gossip pool是不同的，它为具有更高延迟的网络响应做了优化，并且可能包括其他consul集群的server节点。设计WANpool的目的是让数据中心能够以low-touch的方式发现彼此。将一个新的数据中心加入现有的WAN Gossip是很容易的。因为池中的所有Server都是可控制的，这也使跨数据中心的要求。当一个Serfer接收到不同的数据中心的要求时，它把这个请求转发给相应数据中心的任一Server。然后，接收到请求的Server可能会转发给Leader。多个数据中心之间是低耦合，但由于故障检测、连接缓存复用、跨数据中心要求快速和可靠的响应。 3. consul部署3.1 docker安装docker安装很简单，笔者这边是基于docker-compose的配置文件，只需要本地安装好docker和docker-compose，docker-compose.yml如下： 12345678version: '3'services: consul: image: consul ports: - "8500:8500" - "8600:8600" - "8300:8300" 拉取consul得最新image，进行端口映射，暴露对外的端口8500，8300. 3.2 软件安装 从官网下载罪行的consul安装包，https://www.consul.io/downloads.html。 解压consul_0.6.4_darwin_amd64.zip。 将解压后的二进制文件consul拷贝到/usr/local/bin下。 写配置文件。服务注册的配置文件如下: 12345678910111213141516&#123; &quot;service&quot;: &#123; &quot;name&quot;: &quot;redis&quot;, &quot;tags&quot;: [&quot;master&quot;], &quot;address&quot;: &quot;1192.168.1.100&quot;, &quot;port&quot;: 8000, &quot;enableTagOverride&quot;: false, &quot;check&quot;: &#123; &quot;id&quot;: &quot;redis&quot;, &quot;name&quot;: &quot;redis on port 8000&quot;, &quot;tcp&quot;: &quot;localhost:8000&quot;, &quot;interval&quot;: &quot;10s&quot;, &quot;timeout&quot;: &quot;1s&quot; &#125; &#125;&#125; 如上配置注册了Redis的8000端口，并带有tcp的health check。 节点的配置文件： 12345678910111213141516&#123; "datacenter": "east-cn", "data_dir": "/opt/consul", "log_level": "INFO", "node_name": "redis", "server": true, "addresses": &#123; "https": "192.168.1.100" &#125;, "ports": &#123; "https": 0 &#125;, "ui": true, "retry-join": []&#125; 当加载配置选项时，consul是按照词典顺序从所有配置文件或目录中加载。比如，a.json会先于e.json处理。后面设定的配置选项会合并到前面的配置集合中，如果存在重复的配置选项则会覆盖。当然，在某些情况下，比如事件处理程序，后面处理程序会追加到现有的配置选项中，形成事件处理程序列表。 3.3 启动具体启动文档见configuration。如: consul agent -server -config-dir /etc/consul.d -bind=192.168.1.100 -config-dir /etc/consul.d config-dir需要加载的配置文件目录，consul将加载目录下所有后缀为“.json”的文件，加载顺序为字母顺序，文件中配置选项合并方式如config-file。该参数可以多次配置。目录中的子目录是不会加载的。 data-dir此目录是为Agent存放state数据的。是所有Agent需要的，该目录应该存放在持久存储中（reboot不会丢失），对于server角色的Agent是很关键的,需要记录集群状态。并且该目录是支持文件锁。 server设置Agent是server模式还是client模式。Consul agent有两种运行模式：Server和Client。这里的Server和Client只是Consul集群层面的区分，与搭建在Cluster之上 的应用服务无关。Consule Server模式agent节点用于采用raft算法维护Consul集群的状态，官方建议每个Consul Cluster至少有3个或以上的运行在Server mode的Agent，Client节点不限。 其他常用的还有： client将绑定到client接口的地址，可以是HTTP、DNS、RPC服务器。默认为“127.0.0.1”，只允许回路连接。RPC地址会被其他的consul命令使用，比如consul members，查询agent列表 node节点在集群的名字，在集群中必须是唯一的。默认为节点的Hostname。 bootstrap设置服务是否为“bootstrap”模式。如果数据中心只有1个server agent，那么需要设置该参数。从技术上来讲，处于bootstrap模式的服务器是可以选择自己作为Raft Leader的。在consul集群中，只有一个节点可以配置该参数，如果有多个参数配置该参数，那么难以保证一致性。 bind用于集群内部通信的IP地址，与集群中其他节点互连可通。默认为“0.0.0.0”，consul将使用第一个有效的私有IPv4地址。如果指定“[::]”，consul将使用第一个有效的公共IPv6地址。使用TCP和UDP通信。注意防火墙，避免无法通信。 3.4 结果在开启了&quot;ui&quot;: trueserver主机上，如http://192.168.1.100:8500/ui查看注册中心的服务。demo ui如下： 4. 总结本文介绍了consul的一些内幕及consul配置相关，并对项目中的一些实际配置进行展示。希望能够帮助大家对consul相关的知识有所了解，并对于入门配置consul和实际应用有所知道。个人认为，consul原理还是简单易懂的，集群的配置也不复杂，安利大家使用。后面会再写一篇介绍Spring cloud中集成和使用consul组件作为注册与发现中心。 参考文献consul文档consul中文翻译]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>consul</tag>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snowflake升级版全局id生成]]></title>
    <url>%2F2017%2F09%2F09%2Fsnowflake%2F</url>
    <content type="text"><![CDATA[1. 背景分布式系统或者微服务架构基本都采用了分库分表的设计，全局唯一id生成的需求变得很迫切。传统的单体应用，使用单库，数据库中自增id可以很方便实现。分库之后，首先需要分库键，分库键必然不能重复，所以传统的做法并不能满足需求。概括下来，那业务系统对ID号的要求有哪些呢？ 1.全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。2.趋势递增：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。3.单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。4.信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则。 其中第3和第4点是互斥的。除了功能性需求，还有性能和可靠性的需求： 平均延迟和TP999延迟都要尽可能低； 可用性5个9； 高QPS。 2. 进阶历程自从项目从单体应用拆分成微服务架构后，对全局id部分做了些摸索。 2.1 uuid刚开始拆分业务，id主键都是使用uuid字符串。UUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符。类似这样的字符串：dc5adf0a-d531-11e5-95aa-3c15c2d22392。128位，根本不用担心不够用。生成的方法也很简单： 1UUID userId = UUID.randomUUID(); uuid全球唯一，本地生成，没有网络消耗，产生的性能绝对可以满足。其缺点也是显而易见的，比较占地方，和INT类型相比，存储一个UUID要花费更多的空间。使用UUID后，URL显得冗长，不够友好。ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用： MySQL官方有明确的建议主键要尽量越短越好，36个字符长度的UUID不符合要求。 对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能。 2.2 数据库生成以MySQL举例，利用给字段设置auto_increment_increment和auto_increment_offset来保证ID自增，每次业务使用下列SQL读写MySQL得到ID号。参考了Leaf的实现思想: id server每次批量从数据库取号段，本地缓存这个号段，并且设置阈值，当达到0.8（已用与号段容量的比值），自动去获取一个新的号段，更新本地缓存的号段。 id client，即具体的调用服务实例，在本地也做一个缓存，实现和id server的缓存差不多，这样做的目的是为了减轻id服务端的压力，同时减少了rpc调用的网络消耗。 以上方案，其缺点是： 号段存在浪费，无论哪个客户端还是服务端重启都会浪费号段。 号段是直接自增，不够随机，对外暴露信息过多。 DB宕机会造成整个系统不可用。虽然在DB宕机之后，利用缓存还能进行短暂供号，但是数据库的依赖还是很重。Leaf采用的一般做法是高可用容灾: 采用一主两从的方式，同时分机房部署，Master和Slave之间采用半同步方式同步数据。同时使用DBProxy做主从切换。当然这种方案在一些情况会退化成异步模式，甚至在非常极端情况下仍然会造成数据不一致的情况，但是出现的概率非常小。 3. snowflake方案3.1 介绍考虑到上述方案的缺陷，笔者调查了其他的生成方案，snowflake就是其中一种方案。趋势递增和不够随机的问题，在snowflake完全可以解决，Snowflake ID有64bits长，由以下三部分组成： 第一位为0，不用。 timestamp—41bits,精确到ms，那就意味着其可以表示长达(2^41-1)/(1000360024*365)=139.5年，另外使用者可以自己定义一个开始纪元（epoch)，然后用(当前时间-开始纪元）算出time，这表示在time这个部分在140年的时间里是不会重复的，官方文档在这里写成了41bits，应该是写错了。另外，这里用time还有一个很重要的原因，就是可以直接更具time进行排序，对于twitter这种更新频繁的应用，时间排序就显得尤为重要了。 machine id—10bits,该部分其实由datacenterId和workerId两部分组成，这两部分是在配置文件中指明的。 datacenterId，方便搭建多个生成uid的service，并保证uid不重复，比如在datacenter0将机器0，1，2组成了一个生成uid的service，而datacenter1此时也需要一个生成uid的service，从本中心获取uid显然是最快最方便的，那么它可以在自己中心搭建，只要保证datacenterId唯一。如果没有datacenterId，即用10bits，那么在搭建一个新的service前必须知道目前已经在用的id，否则不能保证生成的id唯一，比如搭建的两个uid service中都有machine id为100的机器，如果其server时间相同，那么产生相同id的情况不可避免。 workerId是实际server机器的代号，最大到32，同一个datacenter下的workerId是不能重复的。它会被注册到consul上，确保workerId未被其他机器占用，并将host:port值存入，注册成功后就可以对外提供服务了。 sequence id —12bits,该id可以表示4096个数字，它是在time相同的情况下，递增该值直到为0，即一个循环结束，此时便只能等到下一个ms到来，一般情况下4096/ms的请求是不太可能出现的，所以足够使用了。 3.2 实现思路snowflake方案，id服务端生成，不依赖DB，既能保证性能，且生成的id足够随机。每一毫秒，一台worker可以生成4096个id，如果超过，会阻塞到下一毫秒生成。对于那些并发量很大的系统来说,显然是不够的, 那么这个时候就是通过datacenterId和workerId来做区分,这两个ID,分别是5bit,共10bit,最大值是1024(0-1023)个, 在这种情况下,snowflake一毫秒理论上最大能够生成的ID数量是约42W个,这是一个非常大的基数了,理论上能够满足绝大多数系统的并发量。 该方案依赖于系统时钟，需要考虑时钟回拨的问题。本地缓存上一次请求的lastTimestamp，一个线程过来获取id时，首先校验当前时间是否小于上一次ID生成的时间戳。如果小于说明系统时钟被修改过，回退在上一次ID生成时间之前应当抛出异常！如此可以解决运行中，系统时钟被修改的问题。 另一种情况是，server服务启动时，系统的时间被回拨（虽然比较极端，还是列在考虑中），这样有可能与之前生成的id冲突，全局不唯一。这边解决方法是利用项目的服务发现与注册组件consul，在consul集群存储最新的lastTimestamp，key为对应的machine-id。consul的一致性基于raft算法，并利用Gossip协议： Consul uses a gossip protocol to manage membership and broadcast messages to the cluster. All of this is provided through the use of the Serf library. 具体的协议算法，可以参考Gossip。每次server实例启动时，实例化id生成bean的时候，会首先校验当前时间与consul集群中该worker对应的lastTimestamp大小，如果当前时间偏小，则抛出异常，服务启动失败并报警。 笔者项目暂时未分data center，所以machine-id部分都是以服务实例的workid代替。workid可以从配置中心获取，也可以本地配置。简化的系统架构部署图如下： consul集群这边作为提供naming service和kv存储的组件，每个服务部署后注册到consul集群，至于consul集群相关的信息，以及consul成员的一致性相关，后面单独一篇文章详细介绍。 请求id生成流程图如下： 服务实例启动的流程图见上文，此处不画出了。这边需要强调的是，服务注册与发现组件consul。部署时每个服务实例都会注册到一个consul agent（一般是本机），consul agent连接到consul集群，通过gossip协议进行广播信息，所以如果连接的consul agent进程不幸挂掉（大多为系统宕机），在进程重启后，还是能迅速获取到集群中存储的该workid的lastTimestamp，针对该workid，如果系统时间回拨小于lastTimestamp，Generator启动时会报警。而对于大于lastTimestamp的情况，可能系统时钟还是相对回拨，我们姑且可以认为对全局id没有影响。 实例化时，进行校验： 123456789public IdServiceImpl(long workerId, ConsulClient consulClient) &#123; if (workerId &gt; idMeta.MAX_ID || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0", idMeta.MAX_ID)); &#125; this.workerId = workerId; this.consulClient = consulClient; validateStoredTimestamp(); log.info("worker starting. timestamp left shift &#123;&#125;, worker id bits &#123;&#125;, sequence bits &#123;&#125;, workerid &#123;&#125;", idMeta.TIMESTAMP_LEFT_SHIFT_BITS, idMeta.ID_BITS, idMeta.SEQUENCE_BITS, workerId);&#125; 校验函数： 123456789101112131415/** * checks for timestamp by workerId when server starts. * if server starts for the first time, just let it go and log warns. * if current timestamp is smaller than the value stored in consul server, throw exception. */private void validateStoredTimestamp() &#123; long current = timeGen(); Response&lt;GetValue&gt; keyValueResponse = consulClient.getKVValue(String.valueOf(workerId)); if (keyValueResponse.getValue() != null) &#123; lastTimestamp = Long.parseLong(keyValueResponse.getValue().getDecodedValue()); validateTimestamp(current, lastTimestamp, Periods.START); &#125; else &#123; log.warn(String.format("clock in consul is null. Generator works as for the 1st time.")); &#125;&#125; validateTimestamp: 123456789101112/** * 如果当前时间戳小于上一次ID生成的时间戳，说明系统时钟被修改过，回退在上一次ID生成时间之前应当抛出异常！！！ * * @param lastTimestamp 上一次ID生成的时间戳 * @param timestamp 当前时间戳 */ private void validateTimestamp(long timestamp, long lastTimestamp, Periods period) &#123; if (timestamp &lt; lastTimestamp) &#123; log.error(String.format("clock is moving backwards. Rejecting requests until %d.", lastTimestamp)); throw new IllegalStateException(String.format("Clock moved backwards in %s. Refusing to generate id for %d milliseconds", period, lastTimestamp - timestamp)); &#125; &#125; 获取id方法： 123456789101112131415161718192021222324252627/** * 生成ID（线程安全） * * @return id */ public synchronized long genId() &#123; long timestamp = timeGen(); //如果当前时间小于上一次ID生成的时间戳，说明系统时钟被修改过，回退在上一次ID生成时间之前应当抛出异常！！！ validateTimestamp(timestamp, lastTimestamp, Periods.RUNNING); //如果是同一时间生成的，则进行毫秒内sequence生成 if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; IdMeta.SEQUENCE_MASK; //溢出处理 if (sequence == 0) &#123;//阻塞到下一毫秒,获得新时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123;//时间戳改变，毫秒内sequence重置 sequence = 0L; &#125; //上次生成ID时间截 lastTimestamp = timestamp; consulClient.setKVValue(String.valueOf(workerId), String.valueOf(lastTimestamp)); //移位并通过或运算组成64位ID return ((timestamp - idMeta.START_TIME) &lt;&lt; idMeta.TIMESTAMP_LEFT_SHIFT_BITS) | (workerId &lt;&lt; idMeta.ID_SHIFT_BITS) | sequence; &#125; 4. 总结这篇文章和大家分享了笔者项目中全局id生成服务的演进过程。当前的方案可以满足笔者当前项目的需求，至于分data-center（同一个机房优先调用），需要结合rpc调用进一步做处理，所以这块后续可以继续完善。欢迎大家提出建议。 参考： www.consul.io leaf Twitter的分布式自增ID算法snowflake (Java版)]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>id</tag>
        <tag>spring boot</tag>
        <tag>snowflake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入ThreadLocal]]></title>
    <url>%2F2017%2F09%2F04%2FThreadLocal%2F</url>
    <content type="text"><![CDATA[ThreadLocal主要是提供线程内部的局部变量，在每个线程内随时随地可取，隔离其他线程。 1. ThreadLocal接口1.1 ThreadLocal类接口很简单，只有4个方法，我们先来了解一下： void set(Object value)设置当前线程的线程局部变量的值。 public Object get()该方法返回当前线程所对应的线程局部变量。 public void remove()将当前线程局部变量的值删除，目的是为了减少内存的占用，该方法是JDK 5.0新增的方法。需要指出的是，当线程结束后，对应该线程的局部变量将自动被垃圾回收，所以显式调用该方法清除线程的局部变量并不是必须的操作，但它可以加快内存回收的速度。 protected Object initialValue()返回该线程局部变量的初始值，该方法是一个protected的方法，显然是为了让子类覆盖而设计的。这个方法是一个延迟调用方法，在线程第1次调用get()或set(Object)时才执行，并且仅执行1次。ThreadLocal中的缺省实现直接返回一个null。 在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。 而ThreadLocal则从另一个角度来解决多线程的并发访问。ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。如果想在get之前不需要调用set就能正常访问的话，必须重写initialValue()方法。最常见的ThreadLocal使用场景为 用来解决 数据库连接、Session管理等。 1.2 使用ThreadLocal123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ThreadTest &#123; ThreadLocal&lt;Long&gt; longLocal = new ThreadLocal&lt;Long&gt;()&#123; protected Long initialValue() &#123; return Thread.currentThread().getId(); &#125; &#125;; ThreadLocal&lt;String&gt; stringLocal = new ThreadLocal&lt;String&gt;()&#123;; protected String initialValue() &#123; return Thread.currentThread().getName(); &#125; &#125;; public void set() &#123; longLocal.set(Thread.currentThread().getId()); stringLocal.set(Thread.currentThread().getName()); &#125; public long getLong() &#123; return longLocal.get(); &#125; public String getString() &#123; return stringLocal.get(); &#125; public static void main(String[] args) throws InterruptedException &#123; final ThreadTest test = new ThreadTest(); //test.set(); System.out.println("main.getLong: " + test.getLong()); System.out.println("main.getString: " + test.getString()); Thread thread1 = new Thread() &#123; public void run() &#123; //test.set(); System.out.println("Thread.getLong: " + test.getLong()); System.out.println("Thread.getString: " + test.getString()); &#125; &#125;; thread1.start(); thread1.join(); &#125;&#125; 以上demo覆写了initialValue()方法，或者调用set方法，否则会报空指针异常。在main线程中和thread1线程中，longLocal保存的副本值和stringLocal保存的副本值都不一样。 2. ThreadLocalMapThreadLocalMap的Entry继承了WeakReference，并且使用ThreadLocal作为键值。 1. 实际的通过ThreadLocal创建的副本是存储在每个线程自己的threadLocals中的； 2. 为何threadLocals的类型ThreadLocalMap的键值为ThreadLocal对象，因为每个线程中可有多个threadLocal变量，就像上面代码中的longLocal和stringLocal； 3. 在进行get之前，必须先set，否则会报空指针异常； 2.1 方法分析1). JDK8的ThreadLocal的get方法的源码 123456789101112131415161718192021/** * Returns the value in the current thread's copy of this * thread-local variable. If the variable has no value for the * current thread, it is first initialized to the value returned * by an invocation of the &#123;@link #initialValue&#125; method. * * @return the current thread's value of this thread-local */public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 2). getMap 12345678910/** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 3). setInitialValue 12345678910111213141516/** * Variant of set() to establish initialValue. Used instead * of set() in case user has overridden the set() method. * * @return the initial value */private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; get方法的流程是这样的： 1.首先获取当前线程 2.根据当前线程获取一个Map 3.如果获取的Map不为空，则在Map中以ThreadLocal的引用作为key来在Map中获取对应的value e，否则转到5 4.如果e不为null，则返回e.value，否则转到5 5.Map为空或者e为空，则通过initialValue函数获取初始值value，然后用ThreadLocal的引用和value作为firstKey和firstValue创建一个新的Map 每个Thread维护一个ThreadLocalMap映射表，这个映射表的key是ThreadLocal实例本身，value是真正需要存储的Object。 3. WeakReference关于内存泄露： ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用引用他，那么系统gc的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：ThreadLocal Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value 主要看下getEntryAfterMiss函数： 12345678910111213141516private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null; &#125; ThreadLocalMap的getEntry函数的流程： 首先从ThreadLocal的直接索引位置(通过ThreadLocal.threadLocalHashCode &amp; (len-1)运算得到)获取Entry e，如果e不为null并且key相同则返回e； 如果e为null或者key不一致则向下一个位置查询，如果下一个位置的key和当前需要查询的key相等，则返回对应的Entry，否则，如果key值为null，则擦除该位置的Entry，否则继续向下一个位置查询 在这个过程中遇到的key为null的Entry都会被擦除，那么Entry内的value也就没有强引用链，自然会被回收。仔细研究代码可以发现，set操作也有类似的思想，将key为null的这些Entry都删除，防止内存泄露。 但是光这样还是不够的，上面的设计思路依赖一个前提条件：要调用ThreadLocalMap的genEntry函数或者set函数。这当然是不可能任何情况都成立的，所以很多情况下需要使用者手动调用ThreadLocal的remove函数，手动删除不再需要的ThreadLocal，防止内存泄露。所以JDK建议将ThreadLocal变量定义成private static的，这样的话ThreadLocal的生命周期就更长，由于一直存在ThreadLocal的强引用，所以ThreadLocal也就不会被回收，也就能保证任何时候都能根据ThreadLocal的弱引用访问到Entry的value值，然后remove它，防止内存泄露。 参考：ThreadLocal和synchronized的区别Java并发编程：深入剖析ThreadLocal]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Thread</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自制Jersey-Swagger的spring-boot-starter]]></title>
    <url>%2F2017%2F09%2F02%2Fspring-boot-starter-swaggerforjersey%2F</url>
    <content type="text"><![CDATA[Spring Boot的自动化配置特性来实现快速的将swagger2引入spring boot应用来生成jersey的API文档，简化原生使用swagger2的整合代码。 欢迎使用和Star支持，如使用过程中碰到问题，可以提出Issue，我会尽力完善该Starter 版本基础 Spring Boot：1.5.x swagger-jersey2-jaxrs：2.7.x Jersey 2 如何使用在该项目的帮助下，我们的Spring Boot可以轻松的引入swagger2，主需要做下面两个步骤： 在pom.xml中引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;club.hacloud&lt;/groupId&gt; &lt;artifactId&gt;jersey-starter-swagger&lt;/artifactId&gt; &lt;version&gt;1.0.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; 在应用主类中增加@EnableSwagger2Doc注解 123456789@EnableSwagger2Doc@SpringBootApplicationpublic class Bootstrap &#123; public static void main(String[] args) &#123; SpringApplication.run(Bootstrap.class, args); &#125;&#125; JerseyConfig 中增加1234@PostConstructpublic void init() &#123; this.register(ApiListingResource.class, SwaggerSerializers.class);&#125; 默认情况下就能产生所有当前jersey加载的请求映射文档。 参数配置更细致的配置内容参考如下： 配置示例1234567891011swagger: enabled: true title: spring-boot-starter-swagger config-id: demo-mvc version: v2 license: Apache License, Version 2.0 licenseUrl: https://www.apache.org/licenses/LICENSE-2.0.html termsOfServiceUrl: http://git.oschina.net/keets/jersey-starter-swagger contact: keets base-path: /** resource-package: cn.keets.demo 配置说明默认配置12345678910- swagger.enabled=是否开启 // todo 实现线上关闭功能- swagger.title=标题- swagger.description=描述- swagger.version=版本- swagger.license=许可证- swagger.licenseUrl=许可证URL- swagger.termsOfServiceUrl=服务条款URL- swagger.contact=维护人- swagger.resource-package=swagger扫描的基础包，默认：全扫描- swagger.base-path=需要处理的基础URL规则，默认：/** swagger ui未包含在项目中，大家可以自己部署静态文件，通过静态文件解析json 如下图所示： 项目git地址：http://git.oschina.net/keets/jersey-starter-swaggerdemo git地址：http://git.oschina.net/keets/spring-boot-samples/tree/master/demo-jersey-starter 参考：spring-boot-starter-swagger 1.3.0.RELEASE：新增对JSR-303的支持和host的配置]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>starter</tag>
        <tag>spring-boot</tag>
        <tag>swagger</tag>
        <tag>jersey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Restful Layer of SpringMVC vs Jersey]]></title>
    <url>%2F2017%2F08%2F30%2FJerseyvsspringmvc%2F</url>
    <content type="text"><![CDATA[笔者项目实现前后端剥离，服务端对外提供restful接口。REST逐渐成为影响Web框架、Web协议与Web应用设计的重要概念。现在有越来越多的公司希望能以简单而又贴合Web架构本身的方式公开Web API，因此REST变得越来越重要也就不足为奇了。使用Ajax进行通信的富浏览器端也在朝这个目标不断迈进。这个架构原则提升了万维网的可伸缩性，无论何种应用都能从该原则中受益无穷。SpringMVC和Jersey都可以为你提供restful风格的接口。本文将介绍SpringMVC中的REST特性并与Jersey进行对比。 1. REST基础概念 在REST中的一切都被认为是一种资源。 每个资源由URI标识。 使用统一的接口。处理资源使用POST，GET，PUT，DELETE操作类似创建，读取，更新和删除（CRUD）操作。 无状态。每个请求是一个独立的请求。从客户端到服务器的每个请求都必须包含所有必要的信息，以便于理解。 通信都是通过展现。例如XML，JSON。 2. Jersey与SpringMVCJAX-RS（JSR 311）指的是Java API for RESTful Web Services，Roy Fielding也参与了JAX-RS的制订，他在自己的博士论文中定义了REST。对于那些想要构建RESTful Web Services的开发者来说，JAX-RS给出了不同于JAX-WS（JSR-224）的另一种解决方案。目前共有4种JAX-RS实现，所有这些实现都支持Spring，Jersey则是JAX-RS的参考实现。 有必要指出JAX-RS的目标是Web Services开发（这与HTML Web应用不同）而Spring MVC的目标则是Web应用开发。Spring 3为Web应用与Web Services增加了广泛的REST支持，但本文则关注于与Web Services开发相关的特性。我觉得这种方式更有助于在JAX-RS的上下文中讨论Spring MVC。 要说明的第二点是我们将要讨论的REST特性是Spring Framework的一部分，也是现有的Spring MVC编程模型的延续，因此，并没有所谓的“Spring REST framework”这种概念，有的只是Spring和Spring MVC。这意味着如果你有一个Spring应用的话，你既可以使用Spring MVC创建HTML Web层，也可以创建RESTful Web Services层。]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>REST</tag>
        <tag>Jersey</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac下快速进入当前目录iterm2]]></title>
    <url>%2F2017%2F08%2F28%2Fmac%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[win环境下，有直接在文件浏览的地址上，直接输入cmd，即可打开cmd命令框。笔者在macOS下，也想实现这样的功能，网上查了一下，可以成功实践。 1. 添加服务1git clone https://github.com/peterldowns/iterm2-finder-tools.git 进入 iterm2-finder-tools文件夹，运行iTerm.workflow。安装服务栏。 2. 使用服务在工作文件夹上右键，弹出窗口中找到服务一栏，将鼠标放置其上，在弹出窗口中找到 Open iTerm一栏，单击即可。 3. 添加快捷键服务-&gt;偏好设置-&gt;快捷键 笔者设置了control+command+L。大家可以根据自己的喜好进行设置快捷键。 参考： Mac在Finder中当前目录下打开iTerm2]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>utils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP 2实际应用]]></title>
    <url>%2F2017%2F08%2F27%2Fhttp2%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 背景介绍1.1 需要解决的问题本文来源于项目需要，项目所使用微服务框架为Spring Cloud，微服务之间的调用基于HTTP 1.X协议，上一篇文章 HTTPS vs HTTP 1.1 vs HTTP 2，介绍了http2 和http1.1的相关知识，也列出了http1.1局限性，链路不能复用、数据不加密、头信息过多等等。为此，笔者在想能不能将feign client的调用基于http2协议，做了如下调研。 HTTP/2 源自 SPDY/2。SPDY 系列协议由谷歌开发，于 2009 年公开。它的设计目标是降低 50% 的页面加载时间。当下很多著名的互联网公司，例如百度、淘宝、UPYUN 都在自己的网站或 APP 中采用了 SPDY 系列协议（当前最新版本是 SPDY/3.1），因为它对性能的提升是显而易见的。主流的浏览器（谷歌、火狐、Opera）也都早已经支持 SPDY，它已经成为了工业标准，HTTP Working-Group 最终决定以 SPDY/2 为基础，开发 HTTP/2。2013年8月，进行首次测试，诞生的时间很晚，笔者搜索了网上关于http2实践的相关信息，发现并不多。 1.2 关于项目介绍Spring Cloud是笔者项目采用的微服务框架，具体介绍见Spring Cloud。Spring Cloud是基于Spring Boot开发的组合框架，Spring Boot内置的容器是Tomcat，笔者的项目一般都会exclude Tomcat的引用，使用的是Jetty容器。所以搜索的主题词就变成了 jetty http2。 2. 调研结果大部分的人习惯于将Tomcat运行在8080端口，再用Apache server在前面提供https。这样做是因为简单且验证过的方法。使用http2 ，你将被迫使用https，这样就不用部署Apache (or nginx)。 2.1 服务端 Currently Jetty and undertow are the only servers in Spring Boot that support HTTP/2.Jetty has booked some progress and this repository shows an excellent example. In my opinion it’s still too much custom code, but they’re getting there.The next candidate is undertow. It seems almost too easy, but it works. Because we use AJP in our current configuration it even means this HTTP/2 solution has less lines of code! 当前Spring Boot只有Jetty 和 undertow支持HTTP/2。 样例repo是一个很好的example。总得分为三步： update dependencies org.springframework.boot:spring-boot-starter-undertow org.mortbay.jetty.alpn:alpn-boot:8.1.8.v20160420 create a servlet container bean 1234567@Bean UndertowEmbeddedServletContainerFactory embeddedServletContainerFactory() &#123; UndertowEmbeddedServletContainerFactory factory = new UndertowEmbeddedServletContainerFactory(); factory.addBuilderCustomizers( builder -&gt; builder.setServerOption(UndertowOptions.ENABLE_HTTP2, true)); return factory; &#125; start your server with alpn为了启动服务，需要带上 -Xbootclasspath 参数来包括alpn 。因为alpn 有可能在jdk中没有。 1-Xbootclasspath/p:/home/harrie/.m2/repository/org/mortbay/jetty/alpn/alpn-boot/8.1.8.v20160420/alpn-boot-8.1.8.v20160420.jar 2.2 客户端 Currently Java HTTP/2 clients are scarce. According to this wiki Netty and OkHttp are the only two implementations supported by Spring. To switch HTTP-client in RestTemplate you have to call the constructor with a different ClientHttpRequestFactory (either Netty4ClientHttpRequestFactory or OkHttpClientHttpRequestFactory). 当前Java的http2的客户端也很少，Spring只有Netty and OkHttp支持。这边我们选用了OkHttp，因为OkHttp本来就有在feign client中内置。 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.3.0.RC1&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.3.0.RC1&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt; 2.3 浏览器对于HTTP/2的支持通过浏览器支持http2查看。 2.4 okhttp目前, Http/1.1在全世界大范围的使用中, 直接废弃跳到http/2肯定不现实. 不是每个用户的浏览器都支持http/2的, 也不是每个服务器都打算支持http/2的, 如果我们直接发送http/2格式的协议, 服务器又不支持, 那不是挂掉了! 总不能维护一个全世界的网站列表, 表示哪些支持http/2, 哪些不支持?为了解决这个问题, 从稍高层次上来说, 就是为了更方便地部署新协议, HTTP/1.1 引入了 Upgrade 机制. 这个机制在 RFC7230 的「6.7 Upgrade」这一节中有详细描述.简单说来, 就是先问下你支持http/2么? 如果你支持, 那么接下来我就用http/2和你聊天. 如果你不支持, 那么我还是用原来的http/1.1和你聊天. 客户端在请求头部中指定Connection和Upgrade两个字段发起 HTTP/1.1 协议升级. HTTP/2 的协议名称是 h2c, 代表 HTTP/2 ClearText. 如果服务端不同意升级或者不支持 Upgrade 所列出的协议，直接忽略即可（当成 HTTP/1.1 请求，以 HTTP/1.1 响应）. 如果服务端同意升级，那么需要这样响应 HTTP/1.1 101 Switching ProtocolsConnection: UpgradeUpgrade: h2c[ HTTP/2 connection … ] HTTP Upgrade 响应的状态码是 101，并且响应正文可以使用新协议定义的数据格式。 这样就可以完成从http/1.1升级到http/2了. 同样也可以从http/1.1升级到WebSocket.OkHttp使用了请求协议的协商升级, 无论是1.1还是2, 都先只以1.1来发送, 并在发送的信息头里包含协议升级字段. 接下来就看服务器是否支持协议升级了. OkHttp使用的协议升级字段是ALPN, 如果有兴趣, 可以更深入的查阅相关资料. 3. 总结总体看来，现在Spring boot 是可以支持HTTP/2 server和client。现有项目的api接口面向移动端和web端，web浏览器对于http2的支持在上文已经说明。 参考资料：OkHttp使用完全教程Spring Boot with HTTP/2 – Start a server and make REST calls as a clientHTTPS 与 HTTP2 协议分析]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>HTTP2</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS vs HTTP 1.1 vs HTTP 2]]></title>
    <url>%2F2017%2F08%2F26%2FHTTP2%2F</url>
    <content type="text"><![CDATA[1. HTTPS协议原理分析1.1 需要解决的问题 身份验证:确保通信双方身份的真实性。 通信加密:通信的机密性、完整性依赖于算法与密钥，通信双方是如何选择算法与密钥的。 1.2相关概念 数字证书 CA（certification authority）:数字证书的签发机构。 HTTPS协议、SSL协议、TLS协议、握手协议的关系 HTTPS是Hypertext Transfer Protocol over Secure Socket Layer的缩写，即HTTP over SSL，可理解为基于SSL的HTTP协议。 HTTPS协议安全是由SSL协议（目前常用的，本文基于TLS 1.2进行分析）实现的。 SSL协议是一种记录协议，扩展性良好，可以很方便的添加子协议，而握手协议便是SSL协议的一个子协议。 TLS协议是SSL协议的后续版本，本文中涉及的SSL协议默认是TLS协议1.2版本。 HTTPS协议的安全性由SSL协议实现，当前使用的TLS协议1.2版本包含了四个核心子协议：握手协议、密钥配置切换协议、应用数据协议及报警协议。 1.3 握手协议握手协议的作用便是通信双方进行身份确认、协商安全连接各参数（加密算法、密钥等），确保双方身份真实并且协商的算法与密钥能够保证通信安全。协议交互图： ClientHello消息的作用是，将客户端可用于建立加密通道的参数集合，一次性发送给服务端。 ServerHello消息的作用是，在ClientHello参数集合中选择适合的参数，并将服务端用于建立加密通道的参数发送给客户端。 Certificate消息的作用是，将服务端证书的详细信息发送给客户端，供客户端进行服务端身份校验。 ServerKeyExchange消息的作用是，将需要服务端提供的密钥交换的额外参数，传给客户端。有的算法不需要额外参数，则ServerKeyExchange消息可不发送。 ServerHelloDone消息的作用是，通知客户端ServerHello阶段的数据均已发送完毕，等待客户端下一步消息。 ClientKeyExchange消息的作用是，将客户端需要为密钥交换提供的数据发送给服务端。 ChangeCipherSpec消息的作用，便是声明后续消息均采用密钥加密。在此消息后，我们在WireShark上便看不到明文信息了。 Finished消息的作用，是对握手阶段所有消息计算摘要，并发送给对方校验，避免通信过程中被中间人所篡改。 1.4 总结HTTPS如何保证通信安全，通过握手协议的介绍，我们已经有所了解。但是，在全面使用HTTPS前，我们还需要考虑一个众所周知的问题——HTTPS性能。相对HTTP协议来说，HTTPS协议建立数据通道的更加耗时，若直接部署到App中，势必降低数据传递的效率，间接影响用户体验。 2. HTTP 22.1 HTTP1.x协议随着互联网的快速发展，HTTP1.x协议得到了迅猛发展，但当App一个页面包含了数十个请求时，HTTP1.x协议的局限性便暴露了出来： 每个请求与响应需要单独建立链路进行请求(Connection字段能够解决部分问题)，浪费资源。 每个请求与响应都需要添加完整的头信息，应用数据传输效率较低。 默认没有进行加密，数据在传输过程中容易被监听与篡改。 2.2 HTTP 2介绍HTTP2正是为了解决HTTP1.x暴露出来的问题而诞生的。 说到HTTP2不得不提spdy。由于HTTP1.x暴露出来的问题，Google设计了全新的名为spdy的新协议。spdy在五层协议栈的TCP层与HTTP层引入了一个新的逻辑层以提高效率。spdy是一个中间层，对TCP层与HTTP层有很好的兼容，不需要修改HTTP层即可改善应用数据传输速度。spdy通过多路复用技术，使客户端与服务器只需要保持一条链接即可并发多次数据交互，提高了通信效率。而HTTP2便士基于spdy的思路开发的。通过流与帧概念的引入，继承了spdy的多路复用，并增加了一些实用特性。 新特性： 多路复用 压缩头信息 对请求划分优先级 支持服务端Push消息到客户端 HTTP2目前在实际使用中，只用于HTTPS协议场景下，通过握手阶段ClientHello与ServerHello的extension字段协商而来，所以目前HTTP2的使用场景，都是默认安全加密的。 查看了wiki发现： Netty and OkHttp are the only two implementations supported by Spring. 2.3 协议协商HTTP2协议的协商是在握手阶段进行的。 协商的方式是通过握手协议extension扩展字段进行扩展，新增Application Layer Protocol Negotiation字段进行协商。 在握手协议的ClientHello阶段，客户端将所支持的协议列表填入Application Layer Protocol Negotiation字段，供服务端进行挑选。 2.4 多路复用Multipexing在HTTP2中，同一域名下的请求，可通过同一条TCP链路进行传输，使多个请求不必单独建立链路，节省建立链路的开销。 为了达到这个目的，HTTP2提出了流与帧的概念，流代表请求与响应，而请求与响应具体的数据则包装为帧，对链路中传输的数据通过流ID与帧类型进行区分处理。下图是多路复用的抽象图，每个块代表一帧，而相同颜色的块则代表是同一个流。 归纳下okhttp的多路复用实现思路： 通过请求的Address与连接池中现有连接Address依次匹配，选出可用的Connection。 通过Http2xStream创建的FramedStream在发送了请求后，将FramedStream对象与StreamID的映射关系缓存到FramedConnection中。 收到消息后，FramedConnection解析帧信息，在Map中通过解析的StreamID选出缓存的FramedStream，并唤醒FramedStream进行Response的处理。 2.5 压缩头信息HTTP2为了解决HTTP1.x中头信息过大导致效率低下的问题，提出的解决方案便是压缩头部信息。具体的压缩方式，则引入了HPACK。 HPACK压缩算法是专门为HTTP2头部压缩服务的。为了达到压缩头部信息的目的，HPACK将头部字段缓存为索引，通过索引ID代表头部字段。客户端与服务端维护索引表，通信过程中尽可能采用索引进行通信，收到索引后查询索引表，才能解析出真正的头部信息。 HPACK索引表划分为动态索引表与静态索引表，动态索引表是HTTP2协议通信过程中两端动态维护的索引表，而静态索引表是硬编码进协议中的索引表。 作为分析HPACK压缩头信息的基础，需要先介绍HPACK对索引以及头部字符串的表示方式。 索引 索引以整型数字表示，由于HPACK需要考虑压缩与编解码问题，所以整型数字结构定义下图所示： 类别标识:通过类别标识进行HPACK类别分类，指导后续编解码操作，常见的有1，01，01000000等八个类别。 首字节低位整型:首字节排除类别标识的剩余位，用于表示低位整型。若数值大于剩余位所能表示的容量，则需要后续字节表示高位整型。 结束标识:表示此字节是否为整型解析终止字节。 高位整型:字节余下7bit，用于填充整型高位。]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
        <tag>HTTP2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 集群基础]]></title>
    <url>%2F2017%2F08%2F10%2Fmongodb%E9%9B%86%E7%BE%A4%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1. MongoDB介绍 MongoDB 是一个可扩展的高性能,开源,模式自由,面向文档的数据库。 它使用 C++编写。MongoDB 包含一下特点: 面向集合的存储:适合存储对象及JSON形式的数据。 动态查询:Mongo 支持丰富的查询方式,查询指令使用 JSON 形式的标记,可轻易查询文档中内嵌的对象及数组。 完整的索引支持:包括文档内嵌对象及数组。Mongo 的查询优化器会分析查询表达式,并生成一个高效的查询计划。 查询监视:Mongo包含一个监控工具用于分析数据库操作性能。 复制及自动故障转移:Mongo 数据库支持服务器之间的数据复制,支持主-从模式及服务器之间的相互复制。复制的主要目的是提供冗余及自动故障转移。 高效的传统存储方式:支持二进制数据及大型对象(如:照片或图片)。 自动分片以支持云级别的伸缩性:自动分片功能支持水平的数据库集群,可动态添加额外的机器。 2.Replica Set集群当中包含了多份数据，保证主节点挂掉了，备节点能继续提供数据服务，提供的前提就是数据需要和主节点一致。 Mongodb(M)表示主节点，Mongodb(S)表示备节点，Mongodb(A)表示仲裁节点。主备节点存储数据，仲裁节点不存储数据。客户端同时连接主节点与备节点，不连接仲裁节点。 默认设置下，主节点提供所有增删查改服务，备节点不提供任何服务。但是可以通过设置使备节点提供查询服务，这样就可以减少主节点的压力，当客户端进行数据查询时，请求自动转到备节点上。这个设置叫做Read Preference Modes，同时Java客户端提供了简单的配置方式，可以不必直接对数据库进行操作。 仲裁节点是一种特殊的节点，它本身并不存储数据，主要的作用是决定哪一个备节点在主节点挂掉之后提升为主节点，所以客户端不需要连接此节点。这里虽然只有一个备节点，但是仍然需要一个仲裁节点来提升备节点级别。我开始也不相信必须要有仲裁节点，但是自己也试过没仲裁节点的话，主节点挂了备节点还是备节点，所以咱们还是需要它的。 3.Sharding 和Replica Set类似，都需要一个仲裁节点，但是Sharding还需要配置节点和路由节点。 集群搭建方式首选Replica Set，只有真的是大数据，Sharding才能显现威力，毕竟备节点同步数据是需要时间的。Sharding可以将多片数据集中到路由节点上进行一些对比，然后将数据返回给客户端，但是效率还是比较低的说。 我自己有测试过，不过具体的机器配置已经不记得了。Replica Set的ips在数据达到1400w条时基本能达到1000左右，而Sharding在300w时已经下降到500ips了，两者的单位数据大小大概是10kb。大家在应用的时候还是多多做下性能测试，毕竟不像Redis有benchmark。 Mongodb现在用的还是比较多的，但是个人觉得配置太多了。我看官网都看了好多天，才把集群搭建的配置和注意要点弄明白。而且用过的人应该知道mongodb吃内存的问题，解决办法只能通过ulimit来控制内存使用量，但是如果控制不好的话，mongodb会挂掉 PS: 后面继续补充。]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>Cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud 入门]]></title>
    <url>%2F2017%2F07%2F18%2Fspring-cloud%2F</url>
    <content type="text"><![CDATA[1. 微服务架构微服务架构（Micro-Service Archeticture）是当下流行的架构风格，旨在通过将功能模块分解到各个独立的子系统中以实现解耦，它并没有一成不变的规定，而是需要根据业务来做设计[李贞昊,2017]。微服务架构中，每个微服务模块只是对简单、独立、明确的任务进行处理，通过REST API返回处理结果给外部。在微服务推广实践角度来看，微服务将整个系统进行拆分，拆分成更小的粒度，保持这些服务独立运行，应用容器化技术将微服务独立运行在容器中。过去设计架构时，是在内存中以参数或对象的方式实现粒度细化。微服务使用各个子服务控制模块的思想代替总线。不同的业务要求，服务控制模块至少包含服务的发布、注册、路由、代理功能。 2. vs 单体应用架构微服务架构模式相比于单体应用架构，有很多优势。 首先，巨大的单体式应用拆分为多个微服务，降低了复杂性。在具有之前单体应用功能的同时，单体应用被拆分为多个可管理的微服务。每个微服务都具有定义清楚的边界，使用远程过程调用（RPC）或者消息驱动API。拆分后的微服务模块，粒度小，很容易开发和维护。微服务架构模式降低了单体式编码的难度，并且功能提供了模块化的解决方案。 第二，微服务架构下，专门开发团队负责开发一个子服务。每个开发团队可以自主选择技术栈，提供API接口。当然，许多公司将技术栈统一，只提供特定选择的技术。然后，这种自由使得开发团队不需要被迫使用特定的那些技术，他们可以自由地选择适合该微服务的技术。甚至于，重构之前的代码也变得很便捷。 第三，每个微服务都是独立的部署。开发团队不再需要协调其它服务部署对本服务的影响。这样的特性大大加快了部署速度。微服务架构模式使得持续化部署成为可能。 最后，微服务架构模式使得每个微服务应用都可以被独立扩展。单体架构应用也可以横向扩展，即整个应用完整的复制到不同的节点。当应用的不同组件在扩展需求上存在差异时，微服务架构便体现出其优越性。通过在不同的基础设施之间实现扩展，这些服务能够有效地降低风险[陈春霞, 2016]。 3. Spring Cloud开源框架Spring Cloud是一个基于Spring Boot实现的云应用开发工具，它为基于JVM的云应用开发中的服务发现与注册、熔断机制、路由、全局锁、中心配置管理、控制总线、决策竞选、分布式会话和集群状态管理等操作提供了一种简单的开发方式[翟永超,2016]。Spring Cloud整体架构图如图1.1所示。 Spring Cloud整体架构中如下几个基础服务模块：微服务配置管理、API网关服务、服务发现与注册和消息总线模块。 spring-cloud-config，微服务配置管理，即为上图的config service服务模块，为服务端提供了分布式环境的中央配置支持。配置服务器为各应用的所有环境提供了一个中心化的外部配置。它完成了对服务端Spring-Env和配置源抽象的映射，所以config服务不仅适用于Spring框架构建的应用，也可以使用在其他语言的应用程序。作为一个应用，可以通过部署管道来进行测试或者投入生产，分别为这些环境创建配置，并且在需要迁移环境的时候获取对应的配置来运行。 API网关，本系统使用netflix的zuul框架，作为系统的统一入口，具有负载均衡、服务路由、服务过滤等功能。 服务发现与注册有多种开源组件支持，比如zookeeper、etcd、netflix公司的Eureka，以及本系统使用的Consul。服务发现是一个服务将其地址信息在中心注册节点进行注册的过程。该服务一般会将它的主机IP地址以及端口号进行注册，具体还会包括认证信息、使用协议、版本号等信息，以及关于应用服务环境的细节信息。一个应用服务或者组件通过服务发现可以掌握其运行环境以及其它应用服务或组件的信息。用户配置一个服务发现工具之后，就可以将实际容器与运行配置分离开。]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析REST与HTTP]]></title>
    <url>%2F2017%2F07%2F10%2FREST%E4%B8%8EHTTP%2F</url>
    <content type="text"><![CDATA[幂等性Methods can also have the property of &quot;idempotence&quot; in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. 安全操作与幂指相等特性（Safety /Idempotence）HTTP 的 GET、HEAD 请求本质上应该是安全的调用，即：GET、HEAD 调用不会有任何的副作用，不会造成服务器端状态的改变。对于服务器来说，客户端对某一 URI 做 n 次的 GET、HAED 调用，其状态与没有做调用是一样的，不会发生任何的改变。 HTTP 的 PUT、DELTE 调用，具有幂指相等特性 , 即：客户端对某一 URI 做 n 次的 PUT、DELTE 调用，其效果与做一次的调用是一样的。HTTP 的 GET、HEAD 方法也具有幂指相等特性。HTTP 这些标准方法在原则上保证你的分布式系统具有这些特性，以帮助构建更加健壮的分布式系统。 当然作为设计的基础，几个必须的原则还是要遵守的： 当标准合理的时候遵守标准。 API应该对程序员友好，并且在浏览器地址栏容易输入。 API应该简单，直观，容易使用的同时优雅。 API应该具有足够的灵活性来支持上层ui。 API设计权衡上述几个原则。 HTTPhttp请求由三部分组成，分别是：请求行、消息报头、请求正文. http 1.1/2 http://www.blogjava.net/yongboy/archive/2015/03/23/423751.html HTTP/1.1，HTTP客户端无法重试非幂等请求，尤其在错误发生的时候，由于无法检测错误性质这会对重试带来不利的影响。 HTTP/2不允许使用连接特定头部字段 新增的5个头部 推送机制的一些特性需求 RST_STREAM等帧标志位的使用]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>REST</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式分类]]></title>
    <url>%2F2017%2F01%2F12%2FdesignPattern1%2F</url>
    <content type="text"><![CDATA[1. 设计模式分类 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 其他两类：并发型模式和线程池模式。 这边引用下网上的设计模式图。 2. 设计模式的六大原则2.1 总原则总原则为开闭原则。对扩展开放，对修改封闭。在程序需要进行拓展的时候，不用去修改原有的代码，而是扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。 2.2 单一职责原则不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，否则就应该把类拆分。 2.3 里氏替换原则里氏替换原则（Liskov Substitution Principle），任何基类可以出现的地方，子类一定可以出现。里氏替换原则是继承复用的基石，只有当衍生类可以替换基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。里氏替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 2.4 依赖倒转原则依赖倒转原则（Dependence Inversion Principle），面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 2.5 接口隔离原则接口隔离原则（Interface Segregation Principle），每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 2.6 迪米特法则迪米特法则，最少知道原则（Demeter Principle），一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 2.7 合成复用原则合成复用原则（Composite Reuse Principle），尽量首先使用合成/聚合的方式，而不是使用继承。 3 总结本文主要写了设计模式的总览，对23种设计模式进行分类，包括创建型模式、结构型模式、行为型模式以及两种其他模式。其次介绍了设计模式的六大原则。后面文章将扩展介绍每一种设计模式。 ###参考23种设计模式全解析]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
</search>